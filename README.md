## translate-resnet-paper

### Abstract
深度神经网络的训练难度越来越大。我们提出一个残差学习框架(residual learning framework)，该框架为深度网络的训练提供便利。本文我们证明了残差网络更容易被优化，随着网络深度的增加，准确率可以得到提高。我们创建了一个152层的残差网络，比VGG网络大8倍，但是网络结构很简单。我们在ImageNet数据集上训练该残差网络。我们把多个残差网络结合在一起，在ImageNet测试集上可以达到3.75%的错误率，使得我们在ILSVRC-2015竞赛的分类任务中获得了第一名的成绩。另外我们还在CIFAR-10数据集上测试了100层和1000层的残差网络。
在许多视觉识别任务中，深度是影响网络性能最重要的因素。我们仅仅依靠特别深的网络(100层，1000层)使得我们在COCO 物体检测数据集上相对于其他网络提高了28%的准确率。我们在imagenet检测，定位，COCO检测，COCO分割任务中都获得了第一名的成绩，我们所有的荣誉都归功于深度残差网络。

### Introduction
深度卷积神经网络在图像分类中获得了一系列的突破。深度网络结合了低级/中级/高级的图像特征，网络越深，特征越丰富。最近的研究表明网络深度是一个至关重要的因素。从ImageNet 数据集挑战赛中可以看出，表现最好的网络模型都很深，包含16层甚至30层。许多其他的不属于视觉识别的任务也使用深度模型，并获得了好的效果。

网络越深越好，那么问题来了，对深度网络的训练会遇到什么困难呢？训练深度网络一个最大的障碍是梯度消失和梯度爆炸。通过标准初始化和加入normalization层，使得十多层甚至几十层的网络在SGD随机梯度下降的过程中收敛。

深度网络的收敛问题解决后，接下来是恶化问题degradation problem。随着网络的深度增加，网络的准确率逐渐饱和，之后开始快速恶化。这种恶化不是由过拟合导致的，而是因为添加了更多的层，导致训练集准确率降低。我们的实验证明了这一点，图1就是一个典型的例子。

图1 20层网络和56层网络在CIFAR-10数据集上的训练集错误率和测试集错误率。网络越深，训练集错误率越高，测试集也是如此。在ImageNet数据集上的表现也是如此。
训练集准确率的恶化表明，并不是所有的系统都是那么容易优化。
