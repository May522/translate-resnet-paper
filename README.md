## translate-resnet-paper

### Abstract
深度神经网络的训练难度越来越大。我们提出一个残差学习框架(residual learning framework)，该框架为深度网络的训练提供便利。本文我们证明了残差网络更容易被优化，随着网络深度的增加，准确率可以得到提高。我们创建了一个152层的残差网络，比VGG网络大8倍，但是网络结构很简单。我们在ImageNet数据集上训练该残差网络。我们把多个残差网络结合在一起，在ImageNet测试集上可以达到3.75%的错误率，使得我们在ILSVRC-2015竞赛的分类任务中获得了第一名的成绩。另外我们还在CIFAR-10数据集上测试了100层和1000层的残差网络。
在许多视觉识别任务中，深度是影响网络性能最重要的因素。我们仅仅依靠特别深的网络(100层，1000层)使得我们在COCO 物体检测数据集上相对于其他网络提高了28%的准确率。我们在imagenet检测，定位，COCO检测，COCO分割任务中都获得了第一名的成绩，我们所有的荣誉都归功于深度残差网络。

### Introduction
深度卷积神经网络在图像分类中获得了一系列的突破。深度网络结合了低级/中级/高级的图像特征，网络越深，特征越丰富。最近的研究表明网络深度是一个至关重要的因素。从ImageNet 数据集挑战赛中可以看出，表现最好的网络模型都很深，包含16层甚至30层。许多其他的不属于视觉识别的任务也使用深度模型，并获得了好的效果。

网络越深越好，那么问题来了，对深度网络的训练会遇到什么困难呢？训练深度网络一个最大的障碍是梯度消失和梯度爆炸。通过标准初始化和加入normalization层，使得十多层甚至几十层的网络在SGD随机梯度下降的过程中收敛。

深度网络的收敛问题解决后，接下来是恶化问题degradation problem。随着网络的深度增加，网络的准确率逐渐饱和，之后开始快速恶化。这种恶化不是由过拟合导致的，而是因为添加了更多的层，导致训练集准确率降低。我们的实验证明了这一点，图1就是一个典型的例子。

图1 20层网络和56层网络在CIFAR-10数据集上的训练集错误率和测试集错误率。网络越深，训练集错误率越高，测试集也是如此。在ImageNet数据集上的表现也是如此。

训练集准确率的恶化表明，并不是所有的系统都是那么容易优化。我们首先考虑两个网络，一个浅网络和一个深网络。深网络是在前网络的基础上添加了很多层得到的。对较深的模型的一种解决方案是：添加的这几层是恒等映射，其他的层直接从训练好的浅层网络复制过来。这种解决方案使得一个深的模型相比浅模型的训练集错误率不会升高。但是，实验表明，这种解决方案无法得到我们预期的结果。

本文，我们通过一个深度残差学习框架deep residual learning framework来解决退化问题。与其让几个叠加在一起的层学习出一个恒等映射，我们选择让这些层学习一个残差映射residual mapping。首先，我们把整个残差模块的整体映射记为H(x)，多个层叠加起来的映射为 F(x)=H(x)-x 。我们假设优化残差映射比优化恒等映射更容易。换句话说，如果一个恒等映射达到最优结果，那么残差也就趋于0.

F(x)+x 公式可以使用捷径在前馈神经网络中实现。如图2 所示。捷径就是跳过一个或者多个层级。本文中，捷径只简单表示恒等映射，捷径的输出直接连接到块状网络的输出。这种恒等捷径没有添加额外的参数，也不增加计算量。整个网络仍然可以用SGD进行反馈训练，而且公用的库可以直接拿来用，不需要改变什么。

我们在ImageNet数据集上来呈现深度网络训练中的退化问题，并评估我们的方法。我们将要从以下两方面来展示：1，大型深度残差网络很容易优化。与此相反，‘plain’网络随着深度增加，训练集错误率会提高。2，我们的深度残差网络随着网络深度的增加，训练集错误率很容易就能得到改善。

除了在ImageNet数据集上表现如此，在CIFAR-10数据集上表现也如此。我们的残差网络不只针对于一个数据集，它在不同数据集上的泛化能力很强。

在ImageNet分类数据集上，我们的大型残差深度网络表现很好。我们的152层的残差网络是迄今为止应用在InageNet数据集上最大的网络。虽然我们的网络很大，但是它的复杂度很低，比VGG网络的复杂度要低。我们将多个大型残差网络综合起来，在ImageNet测试集上可以获得3.57% top-5 error，在ILSVRC 2015分类竞赛中获得了第一名。这种大型深度残差网络在其他识别任务中具有很好的泛化能力，使得我们在ImageNet检测，ImageNet定位，COCO检测，COCO分割任务中都获得了第一名的成绩。这足以证明残差学习原理在很多方向都通用，我们期待它在其他视觉和非视觉任务中得到充分应用。

### 2 Related Work
**Residual Representation.** 

**Shortcut Connections.** shortcut connections的实践和理论曾经被研究了很长一段时间。很早的时候，训练multi-layer perceptions(MLPs) 时使用的方法是在输入和输出之间添加一个线性层。有些人直接把几个中间层连接到辅助分类器上，用来解决梯度消失和梯度爆炸的问题。有些文章提出方法
